<h2>Yuhui Xu</h2>

<p>Ph.D <br />
Research Interests: Computer Visionï¼Œ Neural Network Compression <br />
Email: yuhuixu@sjtu.edu.cn <br />
Homepage: <a href="https://yuhuixu1993.github.io/">https://yuhuixu1993.github.io/</a></p>


I am a third year Ph.D. student at Shanghai Jiao Tong University. I am part of the <a href="http://min.sjtu.edu.cn/index.htm">MIN LAB</a>. 
advised by Prof. Hongkai Xiong and Prof. Weiyao Lin. <br />
Prior to SJTU, I obtained my B.S. degree in <a href="http://wjx.seu.edu.cn/wjxen/">
 Chien-Shiung Wu College</a> from Southeast University in 2016.<br />
<hr />

<h2>Publications</h2>

<h3>Conference Papers</h3>

<ul>
<li><p><strong>Deep Neural Network Compression with Single and Multiple Level Qauntization</strong> <br />
<strong>Yuhui Xu</strong>, Yongzhuang Wang, Aojun Zhou, Weiyao Lin, Hongkai Xiong <br />
accepted by <em>32nd Association for the Advancement of Artificial Intelligence Conference on Artificial Intelligence (AAAI18)</em>
[<a href="https://arxiv.org/pdf/1803.03289.pdf">PDF</a>]
[<a href="https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16479">AAAI Digital Library</a>]</p></li>
</ul>

<h3>Preprint</h3>

<ul>
<li><p><strong>DNQ: Dynamic Network Quantization</strong> <br />
<strong>Yuhui Xu</strong>, Shuai Zhang, Yingyong Qi, Jiaxian Guo, Weiyao Lin, Hongkai Xiong <br />
Preprint [<a href="https://arxiv.org/pdf/1812.02375.pdf">PDF</a>]</p></li>
<li><p><strong>Trained Rank Pruning for Efficient Deep Neural Networks</strong> <br />
<strong>Yuhui Xu</strong>, Yuxi Li, Shuai Zhang, Wei Wen, Botao Wang, Yingyong Qi, Yiran Chen, Weiyao Lin, Hongkai Xiong <br />
Preprint [<a href="https://arxiv.org/pdf/1812.02402.pdf">PDF</a>]</p></li>
</ul>

        <div class="media">
                <a name="cp" class="pull-left">
                    <img class="media-object" src="./assets_files/ill-1.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Channel pruning for accelerating very deep neural networks
                     </strong><br>
                        <strong>Yihui He</strong>, Xiangyu Zhang, <a target="_blank" href="http://jiansun.org/">Jian Sun</a>, <strong>ICCV 2017</strong>
                        <a target="_blank"
                           href="http://openaccess.thecvf.com/content_iccv_2017/html/He_Channel_Pruning_for_ICCV_2017_paper.html">[PDF]</a>  
                        <a target="_blank"
                           href="https://arxiv.org/abs/1707.06168">[arXiv]</a>                          
                        <a target="_blank"
                           href="https://raw.githubusercontent.com/yihui-he/images/master/38722_He_0600.png">[Poster]</a>     
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/images/blob/master/channel-pruning-methods.pdf">[Slides]</a>                             
                        <a target="_blank"
                           href="https://github.com/yihui-he/channel-pruning">[Code]</a>       
                       
                    </p>
                    <p class="abstract-text">
                        In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks.Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5x speed-up along with only 0.3% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4%, 1.0% accuracy loss under 2x speed-up respectively, which is significant.
                    </p>
                </div>
            </div> 
